{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DrWatson\n",
    "quickactivate(@__DIR__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove time element and traverse the latent space to create swarming\n",
    "\n",
    "\n",
    "1. Instead of training on windows of data, train only with static clouds\n",
    "2. Traverse latent space to create swarming movement?!\n",
    "3. ...\n",
    "4. Profit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, CSV, DataFrames, MLDataPattern, StatsBase, CUDA, PlotlyJS\n",
    "using BSON: @save\n",
    "using Flux.Data: DataLoader\n",
    "using ProgressMeter: Progress, next!\n",
    "using Flux: logitbinarycrossentropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Base.OneTo(300), Base.OneTo(3), Base.OneTo(3452))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame(CSV.File(\"$(datadir())/exp_raw/data.csv\"; types=Float32))\n",
    "\n",
    "function normalise(M) \n",
    "    min = minimum(minimum(eachcol(M)))\n",
    "    max = maximum(maximum(eachcol(M)))\n",
    "    return (M .- min) ./ (max - min)\n",
    "end\n",
    "\n",
    "normalised = Array(df) |> normalise\n",
    "\n",
    "data = permutedims(reshape(permutedims(normalised, [2,1]),3,300,:), [2,1,3])\n",
    "\n",
    "axes(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_vae (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_vae()\n",
    "    # Define the encoder and decoder networks\n",
    "    encoder_features = Chain(\n",
    "       # 300x3xb\n",
    "        Conv((9,), 3 => 64, relu; stride = 1, pad = SamePad()),\n",
    "        MaxPool((2,)),\n",
    "       # 150x64xb\n",
    "        Conv((5,), 64 => 32, relu; stride = 1, pad = SamePad()),\n",
    "        MaxPool((2,)),\n",
    "        # 75x32xb\n",
    "        Conv((5,), 32 => 12, relu; stride = 1, pad = SamePad()),\n",
    "        MaxPool((3,)),\n",
    "        # 25x12xb\n",
    "        Flux.flatten,\n",
    "        Dense(300, 300, relu)\n",
    "    ) |> gpu \n",
    "        \n",
    "    encoder_μ = Chain(encoder_features, Dense(300, 300)) |> gpu\n",
    "    encoder_logvar = Chain(encoder_features, Dense(300, 300)) |> gpu\n",
    "\n",
    "    decoder = Chain(\n",
    "        Dense(300,300, relu),\n",
    "        (x -> reshape(x, 25, 12, :)),\n",
    "        # 25x12xb\n",
    "        ConvTranspose((5,), 12 => 32, relu; stride = 1, pad = SamePad()),\n",
    "        Upsample(scale=3),\n",
    "        # 75x32xb\n",
    "        ConvTranspose((5,), 32 => 64, relu; stride = 1, pad = SamePad()),\n",
    "        Upsample(scale=2),\n",
    "        # 150x64xb\n",
    "        ConvTranspose((9,), 64 => 3, relu; stride = 1, pad = SamePad()),\n",
    "        Upsample(scale=2)\n",
    "        # 300x3xb\n",
    "    ) |> gpu\n",
    "        \n",
    "    return encoder_μ, encoder_logvar, decoder\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vae_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vae_loss(encoder_μ, encoder_logvar, decoder, x, β, λ)\n",
    "    batch_size = size(x)[end]\n",
    "    @assert batch_size != 0\n",
    "    # Forward propagate through mean encoder and std encoders\n",
    "    μ = encoder_μ(x)\n",
    "    logvar = encoder_logvar(x)\n",
    "    # Apply reparameterisation trick to sample latent\n",
    "    z = μ + gpu(randn(Float32, size(logvar))) .* exp.(0.5f0 * logvar)\n",
    "    # Reconstruct from latent sample\n",
    "    x̂ = decoder(z)\n",
    "    # Negative reconstruction loss Ε_q[logp_x_z]\n",
    "    logp_x_z = -logitbinarycrossentropy(x̂ , x, agg=sum) / batch_size\n",
    "    # KL(qᵩ(z|x)||p(z)) where p(z)=N(0,1) and qᵩ(z|x) models the encoder i.e. reverse KL\n",
    "    # The @. macro makes sure that all operates are elementwise\n",
    "    kl_q_p = 0.5f0 * sum(@. (exp(logvar) + μ^2 - logvar - 1f0)) / batch_size\n",
    "    # Weight decay regularisation term\n",
    "    reg = λ * sum(x->sum(x.^2), Flux.params(encoder_μ, encoder_logvar, decoder))\n",
    "    # We want to maximise the evidence lower bound (ELBO)\n",
    "    elbo = logp_x_z - β .* kl_q_p\n",
    "    # So we minimise the sum of the negative ELBO and a weight penalty\n",
    "    return -elbo + reg\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "save_model (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function save_model(encoder_μ, encoder_logvar, decoder, save_dir::String, epoch::Int)\n",
    "    print(\"Saving model...\")\n",
    "    let encoder_μ = cpu(encoder_μ), encoder_logvar = cpu(encoder_logvar), decoder = cpu(decoder)\n",
    "        @save joinpath(save_dir, \"model-$epoch.bson\") encoder_μ encoder_logvar decoder\n",
    "    end\n",
    "    println(\"Done\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(encoder_μ, encoder_logvar, decoder, dataloader, num_epochs, λ, β, optimiser, save_dir)\n",
    "    # The training loop for the model\n",
    "    trainable_params = Flux.params(encoder_μ, encoder_logvar, decoder)\n",
    "\n",
    "    for epoch_num = 1:num_epochs\n",
    "        acc_loss = 0.0\n",
    "        progress_tracker = Progress(length(dataloader), 1, \"Training epoch $epoch_num: \")\n",
    "        for x_batch in dataloader\n",
    "            \n",
    "            # pullback function returns the result (loss) and a pullback operator (back)\n",
    "            loss, back = Flux.pullback(trainable_params) do\n",
    "                vae_loss(encoder_μ, encoder_logvar, decoder, x_batch |> gpu, β, λ)\n",
    "            end\n",
    "            # Feed the pullback 1 to obtain the gradients and update then model parameters\n",
    "            gradients = back(1f0)\n",
    "            Flux.Optimise.update!(optimiser, trainable_params, gradients)\n",
    "            if isnan(loss)\n",
    "                break\n",
    "            end\n",
    "            acc_loss += loss\n",
    "            next!(progress_traCker; showvalues=[(:loss, loss)])\n",
    "        end\n",
    "        @assert length(dataloader) > 0\n",
    "        avg_loss = acc_loss / length(dataloader)\n",
    "        metrics = DataFrame(epoch=epoch_num, negative_elbo=avg_loss)\n",
    "        CSV.write(joinpath(save_dir, \"metrics.csv\"), metrics, header=(epoch_num==1), append=true)\n",
    "        if epoch_num % 10 == 0 \n",
    "            save_model(encoder_μ, encoder_logvar, decoder, save_dir, epoch_num)\n",
    "        end\n",
    "    end\n",
    "    println(\"Training complete!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Chain(Chain(Conv((9,), 3 => 64, relu, pad=4), MaxPool((2,)), Conv((5,), 64 => 32, relu, pad=2), MaxPool((2,)), Conv((5,), 32 => 12, relu, pad=2), MaxPool((3,)), flatten, Dense(300, 300)), Dense(300, 300)), Chain(Chain(Conv((9,), 3 => 64, relu, pad=4), MaxPool((2,)), Conv((5,), 64 => 32, relu, pad=2), MaxPool((2,)), Conv((5,), 32 => 12, relu, pad=2), MaxPool((3,)), flatten, Dense(300, 300)), Dense(300, 300)), Chain(Dense(300, 300, relu), #3, ConvTranspose((5,), 12 => 32, relu, pad=2), Upsample(:nearest, scale = 3), ConvTranspose((5,), 32 => 64, relu, pad=2), Upsample(:nearest, scale = 2), ConvTranspose((9,), 64 => 3, relu, pad=4), Upsample(:nearest, scale = 2)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32;\n",
    "\n",
    "dataloader = DataLoader(data, batchsize=batch_size, shuffle=true, partial=false)\n",
    "encoder_μ, encoder_logvar, decoder = create_vae()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"results\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "η = 0.0001\n",
    "β = 1f0\n",
    "λ = 0.01f0\n",
    "num_epochs = 500\n",
    "save_dir = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: progress_traCker not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: progress_traCker not defined",
      "",
      "Stacktrace:",
      " [1] train(encoder_μ::Chain{Tuple{Chain{Tuple{Conv{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, MaxPool{1, 2}, Conv{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, MaxPool{1, 2}, Conv{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, MaxPool{1, 2}, typeof(Flux.flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, encoder_logvar::Chain{Tuple{Chain{Tuple{Conv{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, MaxPool{1, 2}, Conv{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, MaxPool{1, 2}, Conv{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, MaxPool{1, 2}, typeof(Flux.flatten), Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, decoder::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, var\"#3#4\", ConvTranspose{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, Upsample{:nearest, Int64, Nothing}, ConvTranspose{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, Upsample{:nearest, Int64, Nothing}, ConvTranspose{1, 2, typeof(relu), Array{Float32, 3}, Vector{Float32}}, Upsample{:nearest, Int64, Nothing}}}, dataloader::DataLoader{Array{Float32, 3}, Random._GLOBAL_RNG}, num_epochs::Int64, λ::Float32, β::Float32, optimiser::ADAM, save_dir::String)",
      "   @ Main ./In[61]:20",
      " [2] top-level scope",
      "   @ In[62]:1",
      " [3] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "train(encoder_μ, encoder_logvar, decoder, dataloader, num_epochs, λ, β, ADAM(η), save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
